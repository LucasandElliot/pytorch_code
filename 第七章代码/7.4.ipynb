{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入本节所需要的模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义文本切分方法，直接使用空格切分即可\n",
    "mytokenize=lambda x: x.split()\n",
    "TEXT=data.Field(sequential=True,tokenize=mytokenize,include_lengths=True,use_vocab=True,\n",
    "batch_first=True,fix_length=200)\n",
    "LABEL=data.Field(sequential=False,use_vocab=False,\n",
    "pad_token=None,unk_token=None)\n",
    "# 对所要读取的数据集和列进行处理\n",
    "train_test_fields=[\n",
    "    (\"label\",LABEL),# 对标签的操作\n",
    "    (\"text\",TEXT)# 对文本的操作\n",
    "    ]\n",
    "# 读取数据\n",
    "traindata,testdata=data.TabularDataset.split(\n",
    "    path=\"./data/chap6\",\n",
    "    format=\"csv\",\n",
    "    train=\"imdb_train.csv\",\n",
    "    fields=train_test_fields,\n",
    "    test=\"imdb_test.csv\",\n",
    "    skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector导入预训练好的词向量文件\n",
    "vec=Vectors(\"glove.6B.100d.txt\",\"./data\")\n",
    "# 使用训练集构建单词表，导入预先训练的词嵌入\n",
    "TEXT.build_vocab(traindata,max_size=20000,vectors=vec)\n",
    "LABEL.build_vocab(traindata)\n",
    "# 训练集，验证集和测试集定义为加载器\n",
    "BATCH_SIZE=32\n",
    "train_iter=data.BucketIterator(traindata,batch_size=BATCH_SIZE)\n",
    "test_iter=data.BucketIterator(testdata,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,layer_dim,output_dim):\n",
    "        \"\"\"\n",
    "        vocab_size:词典长度\n",
    "        embedding_dim:词向量的维度\n",
    "        hidden_dim:GRU神经元个数\n",
    "        layer_dim:GRU的层数\n",
    "        output_dim:隐藏层1输出的维度(分类的数量)\n",
    "        \"\"\"\n",
    "        super(GRUNet,self).__init__()\n",
    "        self.hidden_dim=hidden_dim# GRU神经元个数\n",
    "        self.layer_dim=layer_dim# GRU的层数\n",
    "        # 对文本进行词向量处理\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
    "        # GRU+全连接层\n",
    "        self.gru=nn.GRU(embedding_dim,hidden_dim,layer_dim,batch_first=True)\n",
    "        self.fc1=nn.Sequential(\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,output_dim)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        embeds=self.embedding(x)\n",
    "        # r_out shape (batch,time_step,output_size)\n",
    "        # h_n shape (n_layers,batch,hidden_size)\n",
    "        r_out,h_n=self.gru(embeds,None)# None 表示初始的hidden state 为0\n",
    "        # 选取最后的一个时间点的out输出\n",
    "        out=self.fc1(r_out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化网络\n",
    "vocab_size=len(TEXT.vocab)\n",
    "embedding_dim=vec.dim # 词向量的维度\n",
    "hidden_dim=128\n",
    "layer_dim=1\n",
    "output_dim=2\n",
    "grumodel=GRUNet(vocab_size,embedding_dim,hidden_dim,layer_dim,output_dim)\n",
    "grumodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将导入的词向量作为embedding.weight的初始值\n",
    "grumodel.embedding.weight.data.copy(TEXT.vocab.vectors)\n",
    "# 将无法识别的词'<unk>','<pad>'的向量初始化为0\n",
    "UNK_IDX=TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX=TEXT.vocab.stoi[TEXT.pad_token]\n",
    "grumodel.embedding.weight.data[UNK_IDX]=torch.zeros(vec.dim)\n",
    "grumodel.embedding.weight.data[PAD_IDX]=torch.zeros(vec.dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络的训练过程函数\n",
    "def train_model(model,traindataloader,testdataloader,criterion,optimizer,num_epochs=25):\n",
    "    \"\"\"\n",
    "    model:网络模型,traindataloader:训练数据集,testdataloader:测试数据集\n",
    "    criterion:损失函数,optimizer:优化方法\n",
    "    num_epoch:训练的轮数\n",
    "    \"\"\"\n",
    "    train_loss_all=[]\n",
    "    train_acc_all=[]\n",
    "    test_loss_all=[]\n",
    "    test_acc_all=[]\n",
    "    learn_rate=[]\n",
    "    since=time.time()\n",
    "    # 设置等间距调整学习率，每隔step_size个epoch，学习率缩小到原来的1/10\n",
    "    scheduler=optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        learn_rate.append(scheduler.get_lr()[0])\n",
    "        print('-'*10)\n",
    "        print('Epoch {}/{} ,Lr:{} '.format(epoch,num_epochs-1,learn_rate[-1]))\n",
    "        # 每个epoch有两个阶段:训练阶段和验证阶段\n",
    "        train_loss=0.0\n",
    "        train_corrects=0.0\n",
    "        train_num=0\n",
    "        test_loss=0.0\n",
    "        test_corrects=0\n",
    "        test_num=0\n",
    "        model.train()# 设置模型为训练模型\n",
    "        for step,batch in enumerate(traindataloader):\n",
    "            textdata,target=batch.text[0],batch.label\n",
    "            out=model(textdata)\n",
    "            pre_lab=torch.argmax(out,1)# 预测的标签\n",
    "            loss=criterion(out,target) # 计算损失函数值\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()*len(target)\n",
    "            train_corrects+=torch.sum(pre_lab==target.data)\n",
    "            train_num+=len(target)\n",
    "    # 计算一个epoch在训练集上的损失和精度\n",
    "    train_loss_all.append(train_loss/train_num)\n",
    "    train_acc_all.append(train_corrects.double().item()/train_num)\n",
    "    print('{} Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch,train_loss_all[-1],train_acc_all[-1]))\n",
    "    scheduler.step()# 更新学习率\n",
    "    # 计算一个epoch在验证集上的损失和精度\n",
    "    model.eval()\n",
    "    # 设置模型为评估模型\n",
    "    for step,batch in enumerate(testdataloader):\n",
    "        textdata,target=batch.text[0],batch.label\n",
    "        out=model(textdata)\n",
    "        pre_lab=torch.argmax(out,1)\n",
    "        loss=criterion(out,target)\n",
    "        test_loss+=loss.item()*len(target)\n",
    "        test_corrects+=torch.sum(pre_lab==target.data)\n",
    "        test_num+=len(target)\n",
    "    # 计算一个epoch在训练集的损失和精度\n",
    "    test_loss_all.append(test_loss/test_num)\n",
    "    test_acc_all.append(test_corrects.double().item()/test_num)\n",
    "    print('{} Test Loss: {:.4f} Test Acc: {:.4f}'.format(epoch,test_loss_all[-1],test_acc_all[-1]))\n",
    "    train_process=pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\":range(num_epochs),\n",
    "            \"train_loss_all\":test_loss_all,\n",
    "            \"train_acc_all\":train_acc_all,\n",
    "            \"test_loss_all\":test_loss_all,\n",
    "            \"learn_rate\":learn_rate\n",
    "        })\n",
    "    return model,train_process     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "optimizer=optim.RMSprop(grumodel.parameters(),lr=0.003)\n",
    "loss_func=nn.CrossEntropyLoss()# 交叉熵作为损失函数\n",
    "# 对模型进行迭代训练，对所有的数据训练10轮\n",
    "grumodel,train_process=train_model(\n",
    "    grumodel,train_iter,test_iter,loss_func,optimizer,num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化模型训练过程\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_process.epoch,train_process.train_loss_all,\"r.-\",label=\"Train loss\")\n",
    "plt.plot(train_process.epoch,train_process.test_loss_all,\"bs-\",label=\"Test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch number\",size=13)\n",
    "plt.ylabel(\"Loss value\",size=13)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_process.epoch,train_process.train_acc_all,\"r.-\",label=\"Train acc\")\n",
    "plt.plot(train_process.epoch,train_process.test_acc_all,\"bs-\",label=\"Test acc\")\n",
    "plt.xlabel(\"Epoch number\",size=13)\n",
    "plt.ylabel(\"Acc\",size=13)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对测试集进行预测并计算精度\n",
    "grumodel.eval()# 设置模型为评估模式\n",
    "test_y_all=torch.LongTensor()\n",
    "pre_lab_all=torch.LongTensor()\n",
    "for step,batch in enumerate(test_iter):\n",
    "    textdata,target=batch.text[0],batch.label.view(-1)\n",
    "    out=grumodel(textdata)\n",
    "    pre_lab=torch.argmax(out,1)\n",
    "    test_y_all=torch.cat((test_y_all,target))# 测试集的标签\n",
    "    pre_lab_all=torch.cat((pre_lab_all,pre_lab))# 测试集的预测标签\n",
    "acc=accuracy_score(test_y_all,pre_lab_all)\n",
    "print(\"在测试集上的预测精度为:\",acc)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
