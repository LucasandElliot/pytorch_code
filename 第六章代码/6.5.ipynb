{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入本节需要的模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import time \n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors,GloVe\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义读取训练数据和测试数据的函数\n",
    "def load_text_data(path):\n",
    "    # 获取文件夹最后一个字段\n",
    "    text_data=[]\n",
    "    label=[]\n",
    "    for dest in [\"pos\",\"dest\"]:\n",
    "        path_dest=os.path.join(path,dest)\n",
    "        path_list=os.listdir(path_dest)\n",
    "        # 读取文件夹中的pos或者neg文件\n",
    "        for fname in path_list:\n",
    "            if fname.endswith(\".csv\"):\n",
    "                filename=os.path.join(path_dest,fname)\n",
    "                with open(filename) as f:\n",
    "                    text_data.append(f.read())\n",
    "                if dest==\"pos\":\n",
    "                    label.append(1)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "    # 输出读取的文本和对应的标签\n",
    "    return np.array(text_data),np.array(label)\n",
    "    # 读取训练集和测试集\n",
    "train_path=\"data/chap6/imbd/train\"\n",
    "train_text,train_label=load_text_data(train_path)\n",
    "test_path=\"data/chap6/imdb/test\"\n",
    "test_text,test_label=load_text_data(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对文本进行预处理\n",
    "def text_preprocess(text_data):\n",
    "    text_pre=[]\n",
    "    for text1 in text_data:\n",
    "        # 去除指定的字符\n",
    "        text1=re.sub(\"<br /><br />\",\" \",text1)\n",
    "        # 转化为小写，去除数字，去除标点符号，去除空格\n",
    "        text1=text1.lower()\n",
    "        text1=re.sub(\"\\d+\",\"\",text1)\n",
    "        text1=text1.translate(\n",
    "            str.maketrans(\"\",\"\",string.punctuation.replace(\"'\",\"\"))\n",
    "        )\n",
    "        text1=text1.strip()\n",
    "        text_pre.append(text1)\n",
    "    return np.array(text_pre)\n",
    "train_text_pre=text_preprocess(train_text)\n",
    "test_text_pre=text_preprocess(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-fe83c4e48f01>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-fe83c4e48f01>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    for text in datalist:\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# 文本符号化处理，去除停用词\n",
    "def stop_stem_word(datalist,stop_words):\n",
    "    datalist_pre=[]\n",
    "    for text in datalist:\n",
    "        text_words=word_tokenize(text)\n",
    "        # 去除停用词\n",
    "        text_words=[word for word in text_words if not word in stop_words]\n",
    "        # 删除带有“'”的词语，比如it's\n",
    "        text_words=[word for word in text_words if len(re.findall(\"'\",word))==0]\n",
    "        datalist_pre.append(text_words)\n",
    "    return np.array(datalist_pre)\n",
    "# 文本符号化处理，去除停用词\n",
    "stop_words=stopwords.words(\"english\")\n",
    "stop_words=set(stop_words)\n",
    "train_text_pre2=stop_stem_word(train_text_pre,stop_words)\n",
    "test_text_pre2=stop_stem_word(test_text_pre,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将处理好的文本保存到csv文件中\n",
    "texts=[\"\".join(words) for words in train_text_pre2]\n",
    "traindatasave=pd.DataFrame({\"text\":text,\"label\":train_label})\n",
    "texts=[\"\".join(words) for words in test_text_pre2]\n",
    "testdatasave=pd.DataFrame({\"text\":texts,\"label\":test_label})\n",
    "traindatasave.to_csv(\"data/chap6/imdb.csv\",index=False)\n",
    "testdatasave.to_csv(\"data/chap6/imdb_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将预处理好的文本数据转化为数据表\n",
    "traindata=pd.DataFrame({\"train_text\":train_text,\"train_word\":train_text_pre2,\"train_label\":train_label})\n",
    "# 计算每个影评使用此的数量\n",
    "train_word_num=[len(text) for text in train_text_pre2]\n",
    "traindata[\"train_word_num\"]=train_word_num\n",
    "# 可视化影评词语长度的分布\n",
    "plt.figure(figsize=(8,5))\n",
    "_=plt.hist(train_word_num,bins=100)\n",
    "plt.xlabel(\"word number\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用词云可视化两种情感的词频差异\n",
    "plt.figure(figsize=(16,10))\n",
    "for ii in np.unique(train_label):\n",
    "    # 准备每种情感的所有词语\n",
    "    text=np.array(traindata.train_word[traindata.train_label==ii])\n",
    "    text=\"\".join(np.concatenate(text))\n",
    "    plt.subplot(1,2,ii+1)\n",
    "    # 生成词云\n",
    "    wordcod=WordCloud(margin=5,width=1800,height=1000,max_word=500,min_font_size=5,\n",
    "    background_color='white',max_font=250)\n",
    "    wordcod.generate_from_text(text)\n",
    "    plt.imshow(wordcod)\n",
    "    plt.axis(\"off\")\n",
    "    if ii==1:\n",
    "        plt.title(\"Postitve\")\n",
    "    else:\n",
    "        plt.title(\"Negative\")\n",
    "    plt.subplots_adjust(wspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-1-48b8af23b6d6>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-48b8af23b6d6>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    traindata,testdata=data.TabularDataset.split（\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# 使用torchtext库进行数据准备，定义文件中对文本和标签所要作的操作\n",
    "# 定义文本切分方法，因为前面已经做过处理，使用直接使用空格切分即可\n",
    "mytkoenize=lambda x:x.split()\n",
    "TEXT=data.Field(sequential=True,tokenize=mytkoenize,\n",
    "include_kengths=True,fix_length=200)\n",
    "LABEL=data.Field(\n",
    "    sequential=True,use_vocab=False,\n",
    "    pad_token=None,unk_token=None\n",
    ")\n",
    "# 对所需要读取的数据集的列进行处理\n",
    "train_test_fields=[\n",
    "    (\"label\",LABEL),# 对标签进行处理\n",
    "    (\"text\",TEXT)# 对文本的操作\n",
    "]\n",
    "# 读取数据\n",
    "traindata,testdata=data.TabularDataset.split(\n",
    "    path=\"./data/chap6\",format=\"csv\",\n",
    "    train=\"imdb_train.csv\",fields=train_test_fields,\n",
    "    test=\"imdb_test.csv\",skip_header=True\n",
    ")\n",
    "len(traindata),len(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取样本\n",
    "ex0=traindata.example[0]\n",
    "print(ex0.label)\n",
    "print(ex0.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集切分为训练集和测试集\n",
    "train_data,val_data=traindata.split(split_ratio=0.7)\n",
    "len(train_data),len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练的词向量和构建词汇表\n",
    "vec=Vectors(\"globe.6B.100d.txt\",\"./data\")\n",
    "# 将训练集转化为词向量，使用训练集构建单词表，导入预先训练的词的嵌入\n",
    "TEXT.build_vocab(train_data,max_size=2000,vectors=vec)\n",
    "LABEL.build_vacab(train_data)\n",
    "# 训练集中前10个高频词\n",
    "print(TEXT.vocab.freqs.most_common(n=10))\n",
    "print(\"词典的词数:\",len(TEXT.vocab.itos))\n",
    "peint(\"前10个单词：\\n\",TEXT.vocab.itos[0:10])\n",
    "# 类别标签的数量和类别\n",
    "print(\"类别标签的情况：\",LABEL.vocab.freqs)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-049840c332dd>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-049840c332dd>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    if step>0:\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# 定义一个加载器，将类似长度的示例进行一起批处理\n",
    "BATCH_SIZE=32\n",
    "train_iter=data.BucketIterator(train_data,batch_size=BATCH_SIZE)\n",
    "val_iter=data.BucketIterator(val_data,batch_size=BATCH_SIZE)\n",
    "test_iter=data.BuckIterator(testdata,batch_size=BATCH_SIZE)\n",
    "# 获取一个batch的内容对数据内容进行介绍\n",
    "for step,batch in enumerate(train_iter):\n",
    "    if step>0:\n",
    "        break\n",
    "# 针对一个batch的数据，可以使用batch.label 获取数据的类别标签\n",
    "print(\"数据的类别标签:\\n\",batch.label)\n",
    "# batch.text[0]是文本对应的标签向量\n",
    "print(\"数据的尺寸：\",batch.text[0].shape)\n",
    "# batch.text[1] 对应每个batch使用的原始数据中的索引\n",
    "print(\"数据样本数:\",len(batch.text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.modules):\n",
    "    def __init__(self,vocab_size,embedding_dim,n_filters,filter_sizes,output_dim,\n",
    "    dropout,pad_idx):\n",
    "        super().__init__()\n",
    "        '''vocab_size:词典大小；embedding_dim:词向量维度\n",
    "        n_filters:卷积核的大小，filter_sizes:卷积核的尺寸；\n",
    "        output_dim:输出的维度问题，dropout:dropout的比率\n",
    "        pad_idx:填充的索引'''\n",
    "        # 对文本进行词嵌入\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim,padding_idx=pad_idx)\n",
    "        # 卷积操作\n",
    "        self.convs=nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1,out_channels=n_filters,\n",
    "            kernel_size=(fs,embedding_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        # 全连接层和Dropout层\n",
    "        self.fc=nn.Linear(len(filter_sizes)*n_filters,output_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,text):\n",
    "        #  text=[batch size,sent len]\n",
    "        embedded=self.embedding(text)\n",
    "        # embedded=[batch size,sent len,emb dim]\n",
    "        embedded=embedded.unsequeeze(1)\n",
    "        # embedded=[batch size,1,sent len,emb dim]\n",
    "        conved=[F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        # conved_n=[batch size,n_filters,sent len - filter_size[n]+1]\n",
    "        pooled=[F.max_pool1d(conv,conv.shape[2]).seueeze(2) for conv in conved]\n",
    "        # pooled_n=[batch size,n_filters]\n",
    "        cat=self.dropout(torch.cat(pooled,dim=1))\n",
    "        # cat=[batch size,n_filter*len(filter_sizes)]\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM=len(TEXT.vocab)# 词典的数量\n",
    "EMBEDDING_DIM=100 # 词向量的维度\n",
    "N_FILTERS=100# 每个卷积核的个数\n",
    "FILTER_SIZES=[3,4,5]# 卷积核的高度\n",
    "OUTPUT_DIM=1\n",
    "DROPOUT=0.5\n",
    "PAD_IDX=TEXT.vocab.stoi[TEXT.pad_token]# 填充词的索引\n",
    "model=CNN_Text(INPUT_DIM,EMBEDDING_DIM,N_FILTERS,FILTER_SIZES,OUTPUT_DIM,DROPOUT,PAD_IDX)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将导入的词向量作为embedding.weight的初始值\n",
    "pretrained_embeddings=TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "# 将无法识别的词'<unk>','<pad>'的向量初始化为0\n",
    "UNK_IDX=TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX]=torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX]=torch.zeros(EMBEDDING_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam优化，二分类交叉熵作为损失函数\n",
    "optimizer=optim.Adam(model.parameters())\n",
    "criterion=nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,iterator,optimizer,criterion):\n",
    "    epoch_loss=0;epoch_acc=0\n",
    "    train_corrects=0;train_num=0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        pre=model(batch.text[0]).sequeeze(1)\n",
    "        loss=criterion(pre,batch.label.type(torch.FloatTensor))\n",
    "        pre_lab=torch.round(torch.sigmoid(pre))\n",
    "        train_corrects+=torch.sum(pre_lab.long()==batch.label)\n",
    "        train_num+=len(batch.label)# 样本数量\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "    # 所有样本的平均损失和精度\n",
    "    epoch_loss=epoch_loss/train_num\n",
    "    epoch_acc=train_corrects.double().item()/train_num\n",
    "    return epoch_loss,epoch_acc\n",
    "# 定义一个对数据集验证一轮的函数\n",
    "def evaluate(model,iterator,criterion):\n",
    "    epoch_loss=0;epoch_acc=0\n",
    "    train_corrects=0;train_num=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():# 禁止梯度计算\n",
    "        for batch in iterator:\n",
    "            pre=model(batch.text[0]).sequeeze(1)\n",
    "            loss=criterion(pre,batch.label.type(torch.FloatTensor))\n",
    "            pre_lab=torch.round(torch.sigmoid(pre))\n",
    "            train_corrects+=torch.sum(pre_lab.long()==batch.label)\n",
    "            train_num+=len(batch.label)# 样本数量\n",
    "            epoch_loss+=loss.item()\n",
    "        epoch_loss=epoch_loss/train_num\n",
    "        epoch_acc=train_corrects.double().item()/train_num\n",
    "        return epoch_loss,epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练集训练模型，使用验证集测试模型\n",
    "EPOCH=10\n",
    "best_val_loss=float(\"inf\")\n",
    "best_acc=float(0)\n",
    "for epoch in range(EPOCH):\n",
    "    start_time=time.time()\n",
    "    train_loss,train_acc=train_epoch(model,train_iter,optimizer,criterion)\n",
    "    val_loss,val_acc=evaluate(model,val_iter,criterion)\n",
    "    end_time=time.time()\n",
    "    print(\"Epoch:\",epoch+1,\"|\",\"Epoch Time: \",end_time-start_time,\"s\")\n",
    "    print(\"Train Loss: \",train_loss,\"|\",\"Train Acc: \",train_acc)\n",
    "    print(\"Val Loss: \",val_loss,\"|\",\"Val Acc: \",val_acc)\n",
    "    # 保存效果较好的模型\n",
    "    if(val_loss<best_val_loss)&(val_acc>best_acc):\n",
    "        best_model_wts=copy.deepcopy(model.state_dict())\n",
    "        best_val_loss=val_loss\n",
    "        best_acc=val_acc\n",
    "# 将最好的模型参数重新赋值给model\n",
    "model.load_state_dict(best_model_wts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用evaluate函数对测试集进行预测\n",
    "test_loss,test_acc=evaluate(model,test_iter,criterion)\n",
    "print(\"在测试集上的预测精度为:\",test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
