{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517cd92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.409841Z",
     "start_time": "2022-04-28T07:35:44.153509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "A=torch.tensor([10.0])\n",
    "B=torch.tensor([10.1])\n",
    "# tocrh.allclose为比较接近与否的函数，标准不同所以结果不同\n",
    "print(torch.allclose(A,B,rtol=1e-05,atol=1e-08,equal_nan=False))\n",
    "print(torch.allclose(A,B,rtol=0.1,atol=0.01,equal_nan=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d424ed95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.426103Z",
     "start_time": "2022-04-28T07:35:54.409841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan)\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# equal_nam=True,缺失值可以判断为接近\n",
    "A=torch.tensor(float(\"nan\"))\n",
    "# A被赋值为缺失值\n",
    "print(A)\n",
    "print(torch.allclose(A,A,equal_nan=False))\n",
    "print(torch.allclose(A,A,equal_nan=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083675ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.441861Z",
     "start_time": "2022-04-28T07:35:54.426103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A=torch.tensor([1,2,3,4,5,6])\n",
    "B=torch.arange(1,7)\n",
    "# 计算元素是否相同\n",
    "C=torch.unsqueeze(B,dim=0)\n",
    "print(torch.eq(A,B))\n",
    "print(torch.eq(A,C))\n",
    "# 判断两个张量是否有相同的形状和元素\n",
    "print(torch.equal(A,B))\n",
    "print(torch.equal(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3600ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.457864Z",
     "start_time": "2022-04-28T07:35:54.441861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n",
      "tensor([False, False, False, False, False, False])\n",
      "tensor([[False, False, False, False, False, False]])\n",
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# torch.ge逐个元素比较是否大于等于>=\n",
    "# torch.gt是主元素比较大于>\n",
    "\n",
    "# 逐个元素比较大于等于\n",
    "print(torch.ge(A,B))\n",
    "print(torch.ge(A,C))\n",
    "\n",
    "# 逐个元素比较大于\n",
    "print(torch.gt(A,B))\n",
    "print(torch.gt(A,C))\n",
    "\n",
    "# 逐个元素比较是否小于等于<=\n",
    "print(torch.le(A,B))\n",
    "\n",
    "# 逐个元素比较是否小于<\n",
    "print(torch.lt(A,C))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27df38c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.473868Z",
     "start_time": "2022-04-28T07:35:54.457864Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6]) \n",
      "\n",
      "tensor([1, 2, 3, 4, 5, 6]) \n",
      "\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) \n",
      "\n",
      "tensor([False, False, False, False, False, False]) \n",
      "\n",
      "tensor([[False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# torch.ne()函数就是逐个元素比较不等于\n",
    "print(A,'\\n')\n",
    "print(B,'\\n')\n",
    "print(C,'\\n')\n",
    "print(torch.ne(A,B),'\\n')\n",
    "print(torch.ne(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef8a8b1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.489871Z",
     "start_time": "2022-04-28T07:35:54.473868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.isnan()函数就是逐个元素比较是否有缺失值\n",
    "torch.isnan(torch.tensor([0,1,float(\"nan\"),2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37fd104e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.505877Z",
     "start_time": "2022-04-28T07:35:54.489871Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]]) \n",
      "\n",
      "B: tensor([[10., 12., 14.],\n",
      "        [16., 18., 20.]])\n",
      "tensor([[  0.,  12.,  28.],\n",
      "        [ 48.,  72., 100.]]) \n",
      "\n",
      "tensor([[0.0000, 0.0833, 0.1429],\n",
      "        [0.1875, 0.2222, 0.2500]]) \n",
      "\n",
      "tensor([[10., 13., 16.],\n",
      "        [19., 22., 25.]]) \n",
      "\n",
      "tensor([[-10., -11., -12.],\n",
      "        [-13., -14., -15.]]) \n",
      "\n",
      "tensor([[    inf, 12.0000,  7.0000],\n",
      "        [ 5.3333,  4.5000,  4.0000]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 矩阵逐个元素相乘\n",
    "A=torch.arange(6.0).reshape(2,3)\n",
    "B=torch. linspace(10,20,steps=6).reshape(2,3)\n",
    "print(\"A:\",A,'\\n')\n",
    "print(\"B:\",B)\n",
    "# 逐个元素相乘\n",
    "print(A*B,'\\n')\n",
    "# 逐个元素相除\n",
    "print(A/B,'\\n')\n",
    "# 逐个元素相加\n",
    "print(A+B,'\\n')\n",
    "# 逐个元素相减\n",
    "print(A-B,'\\n')\n",
    "# 逐个元素相除\n",
    "print(B/A,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e6a4910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.533235Z",
     "start_time": "2022-04-28T07:35:54.505877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]]) \n",
      "\n",
      "B: tensor([[10., 12., 14.],\n",
      "        [16., 18., 20.]])\n",
      "tensor([[  0.,   1.,   8.],\n",
      "        [ 27.,  64., 125.]])\n",
      "tensor([[  0.,   1.,   8.],\n",
      "        [ 27.,  64., 125.]])\n",
      "tensor([[  1.0000,   2.7183,   7.3891],\n",
      "        [ 20.0855,  54.5981, 148.4132]])\n",
      "tensor([[  -inf, 0.0000, 0.6931],\n",
      "        [1.0986, 1.3863, 1.6094]])\n",
      "tensor([[0.0000, 1.0000, 1.4142],\n",
      "        [1.7321, 2.0000, 2.2361]])\n",
      "tensor([[0.0000, 1.0000, 1.4142],\n",
      "        [1.7321, 2.0000, 2.2361]])\n",
      "tensor([[   inf, 1.0000, 0.7071],\n",
      "        [0.5774, 0.5000, 0.4472]])\n",
      "tensor([[   inf, 1.0000, 0.7071],\n",
      "        [0.5774, 0.5000, 0.4472]])\n"
     ]
    }
   ],
   "source": [
    "print(\"A:\",A,'\\n')\n",
    "print(\"B:\",B)\n",
    "# 张量的幂\n",
    "print(torch.pow(A,3))\n",
    "print(A**3)\n",
    "# 计算张量的指数\n",
    "print(torch.exp(A))\n",
    "# 计算张量的对数\n",
    "print(torch.log(A))\n",
    "# 计算张量的平方根\n",
    "print(torch.sqrt(A))\n",
    "print(A**0.5)\n",
    "# 计算张量的平方根倒数\n",
    "print(torch.rsqrt(A))\n",
    "print(1/(A**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e6fe66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.549249Z",
     "start_time": "2022-04-28T07:35:54.533235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]]) \n",
      "\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 4.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[2.5000, 2.5000, 2.5000],\n",
      "        [3.0000, 4.0000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 根据最大值裁剪torch.clamp_max(),根据最小值裁剪torch.clamp_min()和范围裁剪torch.clamp()\n",
    "\n",
    "print(\"A:\",A,\"\\n\")\n",
    "# 根据最大裁剪\n",
    "print(torch.clamp_max(A,4))\n",
    "# 根据最小值裁剪\n",
    "print(torch.clamp_min(A,3))\n",
    "#根据范围裁剪\n",
    "print(torch.clamp(A,2.5,4))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e85859ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.581245Z",
     "start_time": "2022-04-28T07:35:54.549249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]]) \n",
      "\n",
      "tensor([[0., 3.],\n",
      "        [1., 4.],\n",
      "        [2., 5.]])\n",
      "tensor([[ 5., 14.],\n",
      "        [14., 50.]])\n",
      "tensor([[[ 10.,  13.],\n",
      "         [ 28.,  40.]],\n",
      "\n",
      "        [[172., 193.],\n",
      "         [244., 274.]]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n"
     ]
    }
   ],
   "source": [
    "# 对于张量矩阵的一些运算\n",
    "A=torch.arange(6.0).reshape(2,3)\n",
    "print(\"A:\",A,\"\\n\")\n",
    "# 矩阵的转置\n",
    "C=torch.t(A)\n",
    "print(C)\n",
    "# 矩阵运算，矩阵相乘，A的行数要等于C的列数\n",
    "print(A.matmul(C))\n",
    "A=torch.arange(12.0).reshape(2,2,3)\n",
    "B=torch.arange(12.0).reshape(2,3,2)\n",
    "AB=torch.matmul(A,B)\n",
    "print(AB)\n",
    "# 矩阵相乘只计算最后两个维度的乘法\n",
    "print(AB[0].eq(torch.matmul(A[0],B[0])))\n",
    "print(AB[1].eq(torch.matmul(A[1],B[1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eabb0a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.597238Z",
     "start_time": "2022-04-28T07:35:54.581245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: tensor([[0.9099, 0.1136, 0.9527],\n",
      "        [0.9357, 0.1133, 0.1987],\n",
      "        [0.3972, 0.3088, 0.3452]]) \n",
      "\n",
      "tensor([[ 1.0000e+00,  0.0000e+00,  4.6566e-08],\n",
      "        [ 0.0000e+00,  1.0000e+00, -1.6298e-09],\n",
      "        [ 2.9802e-08,  2.9802e-08,  1.0000e+00]])\n",
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "C=torch.rand(3,3)\n",
    "# 计算矩阵的逆矩阵，而且用inversehanhsu1\n",
    "D=torch.inverse(C)\n",
    "print(\"C:\",C,\"\\n\")\n",
    "print(torch.mm(C,D))\n",
    "# 计算对角线元素的和，而且对角线元素的和成为矩阵的迹\n",
    "print(torch.trace(torch.arange(9.0).reshape(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8aecf66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.613253Z",
     "start_time": "2022-04-28T07:35:54.597238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大值: tensor(99.)\n",
      "最大值位置： tensor(8)\n",
      "最小值: tensor(11.)\n",
      "最小值位置： tensor(3)\n",
      "2-D张量B:\n",
      " tensor([[12., 34., 25., 11.],\n",
      "        [67., 32., 29., 30.],\n",
      "        [99., 55., 23., 44.]])\n",
      "最大值：\n",
      " torch.return_types.max(\n",
      "values=tensor([34., 67., 99.]),\n",
      "indices=tensor([1, 0, 0]))\n",
      "最大值的位置： tensor([1, 0, 0])\n",
      "最小值:\n",
      " torch.return_types.min(\n",
      "values=tensor([12., 32., 23., 11.]),\n",
      "indices=tensor([0, 1, 2, 0]))\n",
      "最小值的位置： tensor([0, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "A=torch.tensor([12.,34,25,11,67,32,29,30,99,55,23,44])\n",
    "# 最大值以及最大值的位置\n",
    "print(\"最大值:\",A.max())\n",
    "print(\"最大值位置：\",A.argmax())\n",
    "# 最小值以及最小值的位置\n",
    "print(\"最小值:\",A.min())\n",
    "print(\"最小值位置：\",A.argmin())\n",
    "B=A.reshape(3,4)\n",
    "# 二维张量的最大值以及最小值\n",
    "print(\"2-D张量B:\\n\",B)\n",
    "# 最大值以及每行最大值的位置\n",
    "print(\"最大值：\\n\",B.max(dim=1))\n",
    "print(\"最大值的位置：\",B.argmax(dim=1))\n",
    "# 最小值以及位置（每列）\n",
    "print(\"最小值:\\n\",B.min(dim=0))\n",
    "print(\"最小值的位置：\",B.argmin(dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ef28b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.629257Z",
     "start_time": "2022-04-28T07:35:54.613253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([12., 34., 25., 11., 67., 32., 29., 30., 99., 55., 23., 44.]) \n",
      "\n",
      "torch.return_types.sort(\n",
      "values=tensor([11., 12., 23., 25., 29., 30., 32., 34., 44., 55., 67., 99.]),\n",
      "indices=tensor([ 3,  0, 10,  2,  6,  7,  5,  1, 11,  9,  4,  8]))\n",
      "torch.return_types.sort(\n",
      "values=tensor([99., 67., 55., 44., 34., 32., 30., 29., 25., 23., 12., 11.]),\n",
      "indices=tensor([ 8,  4,  9, 11,  1,  5,  7,  6,  2, 10,  0,  3]))\n",
      "B: tensor([[12., 34., 25., 11.],\n",
      "        [67., 32., 29., 30.],\n",
      "        [99., 55., 23., 44.]]) \n",
      "\n",
      "B sort:\n",
      " tensor([[11., 12., 25., 34.],\n",
      "        [29., 30., 32., 67.],\n",
      "        [23., 44., 55., 99.]])\n",
      "B sort index:\n",
      " tensor([[3, 0, 2, 1],\n",
      "        [2, 3, 1, 0],\n",
      "        [2, 3, 1, 0]])\n",
      "B argsort:\n",
      " tensor([[3, 0, 2, 1],\n",
      "        [2, 3, 1, 0],\n",
      "        [2, 3, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 对于以为张量进行排序，或者对于高位张量的一个指定维度排序，\n",
    "# 按照升序排列\n",
    "print(\"A:\",A,\"\\n\")\n",
    "print(torch.sort(A))\n",
    "# 按照降序排列\n",
    "print(torch.sort(A,descending=True))\n",
    "# 对2D张量进行排序\n",
    "print(\"B:\",B,\"\\n\")\n",
    "Bsort,Bsort_id=torch.sort(B)\n",
    "print(\"B sort:\\n\",Bsort)\n",
    "print(\"B sort index:\\n\",Bsort_id)\n",
    "print(\"B argsort:\\n\",torch.argsort(B))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f32430b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:35:54.641251Z",
     "start_time": "2022-04-28T07:35:54.629257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B:\n",
      " tensor([[12., 34., 25., 11.],\n",
      "        [67., 32., 29., 30.],\n",
      "        [99., 55., 23., 44.]])\n",
      "B 每列top2:\n",
      " tensor([[99., 55., 29., 44.],\n",
      "        [67., 34., 25., 30.]])\n",
      "B top2 的位置\n",
      " tensor([[2, 2, 1, 2],\n",
      "        [1, 0, 0, 1]])\n",
      "torch.return_types.kthvalue(\n",
      "values=tensor(23.),\n",
      "indices=tensor(10))\n",
      "torch.return_types.kthvalue(\n",
      "values=tensor([25., 32., 55.]),\n",
      "indices=tensor([2, 1, 1]))\n",
      "Bktn: tensor([[25.],\n",
      "        [32.],\n",
      "        [55.]])\n"
     ]
    }
   ],
   "source": [
    "# 获取张量前几个大的数值\n",
    "torch.topk(A,4)\n",
    "# 获取2-D张量每列前几个大的数值\n",
    "print(\"\\nB:\\n\",B)\n",
    "Btop2,Btop2_id=torch.topk(B,2,dim=0)\n",
    "print(\"B 每列top2:\\n\",Btop2)\n",
    "print(\"B top2 的位置\\n\",Btop2_id)\n",
    "# 获取张量第k小的数值以及位置\n",
    "print(torch.kthvalue(A,3))\n",
    "# 获取2--D张量第k小的数值以及位置\n",
    "print(torch.kthvalue(B,3,dim=1))\n",
    "# 获取张量第k小的数值以及位置\n",
    "Bkth,Bkth_id=torch.kthvalue(B,3,dim=1,keepdim=True)\n",
    "print(\"Bktn:\",Bkth)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d1c7d15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T08:13:29.246575Z",
     "start_time": "2022-04-28T08:13:29.236572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: tensor([[12., 34., 25., 11.],\n",
      "        [67., 32., 29., 30.],\n",
      "        [99., 55., 23., 44.]]) \n",
      "\n",
      "tensor([[20.5000],\n",
      "        [39.5000],\n",
      "        [55.2500]]) torch.Size([3, 1])\n",
      "tensor([20.5000, 39.5000, 55.2500]) torch.Size([3])\n",
      "tensor([[59.3333, 40.3333, 25.6667, 28.3333]])\n",
      "tensor([[ 82.],\n",
      "        [158.],\n",
      "        [221.]])\n",
      "tensor([[178., 121.,  77.,  85.]])\n",
      "tensor([[ 12.,  46.,  71.,  82.],\n",
      "        [ 67.,  99., 128., 158.],\n",
      "        [ 99., 154., 177., 221.]])\n",
      "tensor([[ 12.,  34.,  25.,  11.],\n",
      "        [ 79.,  66.,  54.,  41.],\n",
      "        [178., 121.,  77.,  85.]])\n",
      "torch.return_types.median(\n",
      "values=tensor([[12.],\n",
      "        [30.],\n",
      "        [44.]]),\n",
      "indices=tensor([[0],\n",
      "        [3],\n",
      "        [3]]))\n",
      "torch.return_types.median(\n",
      "values=tensor([[67., 34., 25., 30.]]),\n",
      "indices=tensor([[1, 0, 0, 1]]))\n"
     ]
    }
   ],
   "source": [
    "# 平均值，计算每行的均值\n",
    "print(\"B:\",B,\"\\n\")\n",
    "print(torch.mean(B,dim=1,keepdim=True),torch.mean(B,dim=1,keepdim=True).size())\n",
    "print(torch.mean(B,dim=1,keepdim=False),torch.mean(B,dim=1,keepdim=False).size())\n",
    "# 平均值，计算每列的均值\n",
    "print(torch.mean(B,dim=0,keepdim=True))\n",
    "# 计算每行的和\n",
    "print(torch.sum(B,dim=1,keepdim=True))\n",
    "# 计算每列的和\n",
    "print(torch.sum(B,dim=0,keepdim=True))\n",
    "# 按照行计算累加和\n",
    "print(torch.cumsum(B,dim=1))\n",
    "# 按照列计算累加和\n",
    "print(torch.cumsum(B,dim=0))\n",
    "# 计算每行的中位数\n",
    "print(torch.median(B,dim=1,keepdim=True))\n",
    "# 计算每列的中位数\n",
    "print(torch.median(B,dim=0,keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "041b7443",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T08:39:07.479753Z",
     "start_time": "2022-04-28T08:39:07.460517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 112200.],\n",
      "        [1865280.],\n",
      "        [5510340.]])\n",
      "tensor([[79596., 59840., 16675., 14520.]])\n",
      "tensor([[1.2000e+01, 4.0800e+02, 1.0200e+04, 1.1220e+05],\n",
      "        [6.7000e+01, 2.1440e+03, 6.2176e+04, 1.8653e+06],\n",
      "        [9.9000e+01, 5.4450e+03, 1.2524e+05, 5.5103e+06]])\n",
      "tensor([[1.2000e+01, 3.4000e+01, 2.5000e+01, 1.1000e+01],\n",
      "        [8.0400e+02, 1.0880e+03, 7.2500e+02, 3.3000e+02],\n",
      "        [7.9596e+04, 5.9840e+04, 1.6675e+04, 1.4520e+04]])\n",
      "tensor(25.0108)\n"
     ]
    }
   ],
   "source": [
    "# 按照计行计算乘积\n",
    "print(torch.prod(B,dim=1,keepdim=True))\n",
    "# 按照列计算乘积\n",
    "print(torch.prod(B,dim=0,keepdim=True))\n",
    "# 按照行计算累乘积\n",
    "print(torch.cumprod(B,dim=1))\n",
    "# 按照列计算累乘积\n",
    "print(torch.cumprod(B,dim=0))\n",
    "# 标准差\n",
    "print(torch.std(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20523703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 实现任意标量值函数自动求导的类和函数\n",
    "# 针对一个张量只需要设置参数requires_grad,即可输出传播过程中的梯度信息\n",
    "x=torch.tensor([[1.0,2.0],[3.0,4.0]],requires_grad=True)\n",
    "# 默认require_grad=False\n",
    "y=torch.sum(x**2+2*x+1)\n",
    "# y 的导数是2x+2\n",
    "print(\"X.requires_grad:\",x.requires_grad)\n",
    "print(\"y.requires_grad:\",y.requires_grad)\n",
    "print(\"x:\",x)\n",
    "print(\"y:\",y)\n",
    "# 计算y在x的梯度\n",
    "print(y.backward())\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fee741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
